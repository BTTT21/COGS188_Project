{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Haoyu(Eric) Wang\n",
    "- Gexiang(Jason) Zhang\n",
    "- Bryant Zhu\n",
    "- Jiachen Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured\n",
    "\n",
    "Our project aims to develop an advanced AI system to efficiently solve 16x32 Minesweeper by implementing Constraint Satisfaction Problem (CSP) techniques, Monte Carlo simulations, and Reinforcement Learning (RL). Unlike toy problems, Minesweeper presents a complex action space with both deterministic logic-based decisions and probabilistic uncertainty, making it a compelling testbed for AI methods.\n",
    "\n",
    "Our AI system will be evaluated using multiple performance metrics, including completion rate, accuracy, and computational efficiency, to identify the most effective combination of techniques. We will also explore hyperparameter tuning to optimize model performance. The results will provide insights into the strengths and limitations of each approach, contributing to a deeper understanding of AI-driven problem-solving in Minesweeper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Minesweeper is a classic combinatorial puzzle that has been widely studied in artificial intelligence (AI) due to its mix of deterministic and probabilistic decision-making elements. The game is classified as an NP-complete problem, meaning that there is no known polynomial-time algorithm that can efficiently solve all instances of the problem (Kaye, 2000)[1]. This computational complexity makes Minesweeper an interesting challenge for AI research, particularly in areas such as constraint satisfaction, reinforcement learning, and probabilistic reasoning.\n",
    "#### **1. AI Approaches to Minesweeper**\n",
    "Several AI techniques have been applied to solving Minesweeper. One common approach is Constraint Satisfaction Problems (CSPs), where each revealed number provides constraints on adjacent hidden cells. Techniques such as backtracking search and forward checking have been effective in determining safe moves deterministically (Littman et al., 2002)[2]. However, CSP-based methods struggle in situations where the game state forces a guess, which often occurs in Minesweeper due to its probabilistic nature.\n",
    "\n",
    "Another approach involves Monte Carlo simulations, where AI agents simulate multiple possible game states and estimate probabilities of mine locations (Chin et al., 2019)[3]. This method enables the AI to make informed probabilistic decisions in uncertain situations, but it can be computationally expensive, especially in large grid settings.\n",
    "\n",
    "Reinforcement Learning (RL) has also been explored as a potential solution, where agents learn optimal decision policies through trial and error. Researchers have applied deep Q-networks (DQN) and policy-based learning methods to train AI agents that can navigate Minesweeper efficiently (Wu & Baldi, 2020)[4]. However, training RL agents can be challenging due to the sparse reward structure of the game—rewards are only provided upon completion of the game or upon uncovering a mine, making learning slow and unstable. \n",
    "\n",
    "#### **2. Significance of Our Approach**\n",
    "Our project aims to integrate CSP, Monte Carlo simulations, and Dynamic Programming (DP) to develop a more robust Minesweeper-solving AI. By leveraging CSP for logical inference, Monte Carlo simulations for probabilistic decision-making, and DP for optimal policy derivation, we aim to develop an AI system that efficiently balances logic-based deduction with probabilistic reasoning. This hybrid approach could provide new insights into AI problem-solving strategies in uncertain environments and contribute to broader applications in AI planning and decision-making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "**1. Brief Rules:**  \n",
    "In Minesweeper, you are presented with a grid where certain cells hide mines. The objective is to reveal all safe cells without uncovering any mines. When a safe cell is revealed, it displays a number that indicates how many of its adjacent cells contain mines; if the number is zero, a cascade effect automatically reveals its neighboring cells. Players can also flag cells they suspect to contain mines to avoid accidental clicks.\n",
    "\n",
    "**2. Environment, Actions, Transition, Reward and Goal:**  \n",
    "The game is played on a 16*30 grid with a predetermined number of mines randomly placed at the start. The player interacts with the environment by choosing to reveal a cell at given coordinates or flag/unflag a cell as potentially dangerous. When a cell is revealed, if it is safe, the board updates to show the number of adjacent mines; if that number is zero, it triggers a cascade reveal of neighboring cells. However, revealing a mine immediately ends the game in a loss. The reward structure typically offers small rewards for safe moves and a significant positive reward for successfully uncovering all safe cells (winning), while revealing a mine results in a large negative reward (losing). The overall goal is to strategically reveal every safe cell without triggering any mines, leveraging logical deduction and, when necessary, probabilistic reasoning.\n",
    "\n",
    "**3. Potential Challenges:**  \n",
    "The main challenges include dealing with partial observability, as the true locations of mines remain hidden until revealed, and handling ambiguous situations that may force the player to guess. Additionally, Minesweeper is computationally complex (NP-complete), and the sparse reward structure can make it difficult for algorithmic or learning-based approaches to effectively deduce optimal moves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "In this project, we will apply **three distinct methods**—**Dynamic Programming (DP)**, **Monte Carlo (MC) simulations**, and **Constraint Satisfaction Problem (CSP)** techniques—to solve the **16x32 Minesweeper** puzzle. Each of these methods will be implemented independently, and their performances will be compared to determine which approach provides the most efficient and accurate solution.\n",
    "\n",
    "### **1. Dynamic Programming (DP)**\n",
    "\n",
    "We will model the Minesweeper grid as a **Markov Decision Process (MDP)**, where the state of the game is represented by the configuration of tiles (either mines or safe spaces). The objective is to uncover safe tiles while avoiding mines, guided by the numeric clues provided on the grid.\n",
    "\n",
    "The core of our approach is based on **Value Iteration** and **Policy Iteration** methods:\n",
    "- **Value Iteration:** This algorithm iteratively updates the value of each state (tile) based on its neighboring states and the rewards (whether it's a safe tile or a mine). It computes the expected value of uncovering each tile, considering both deterministic and probabilistic outcomes.\n",
    "- **Policy Iteration:** After estimating the values of states, this algorithm will derive the optimal policy—i.e., the best action (whether to uncover or skip a tile) to maximize the chances of winning.\n",
    "\n",
    "**Why It Works:**  \n",
    "DP is highly effective in environments where the transitions between states are known and can be modeled deterministically. By iterating over the grid, DP can ensure optimal decision-making by considering the entire game state at once.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Monte Carlo (MC) Simulations**\n",
    "\n",
    "Monte Carlo simulations will be used to handle the probabilistic nature of Minesweeper. The idea is to simulate multiple random games (trajectories) and use the results to estimate the most likely safe moves. This will involve the following steps:\n",
    "- **Random Sampling:** At each step, a random decision will be made based on the probabilities of safe tiles. This is akin to exploring various possible states the game could be in, given partial information.\n",
    "- **Exploration vs. Exploitation:** The algorithm will balance between exploring unknown tiles (exploration) and exploiting the information from already uncovered tiles (exploitation).\n",
    "\n",
    "**Why It Works:**  \n",
    "MC simulations are particularly useful in solving problems with uncertainty, as they approximate the solution by simulating a large number of possible scenarios. By performing many simulations, the algorithm can generate an accurate estimate of the most probable outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Constraint Satisfaction Problem (CSP)**\n",
    "\n",
    "Minesweeper can be modeled as a **Constraint Satisfaction Problem (CSP)**, where each tile is a variable with a domain of {Mine, Safe}. The constraints come from the numerical clues on each uncovered tile, indicating the number of mines in the adjacent tiles. Our approach will be based on **backtracking** and **forward checking** to efficiently search for a solution while satisfying the constraints.\n",
    "\n",
    "- **Backtracking:** This algorithm will systematically explore all possible configurations of safe and mine tiles, backtracking when a configuration violates a constraint.\n",
    "- **Forward Checking:** After each decision, forward checking will be used to prune the search space by eliminating values from the domains of adjacent variables that cannot possibly satisfy the constraints.\n",
    "\n",
    "**Why It Works:**  \n",
    "CSPs are a natural fit for Minesweeper because the game's constraints (the numbers on the uncovered tiles) must be satisfied while assigning values (Mine or Safe) to the tiles. Backtracking and forward checking are both well-established techniques for solving CSPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "To evaluate the performance of our Minesweeper AI agents, we propose several evaluation metrics that reflect accuracy and efficiency. \n",
    "\n",
    "* The win rate will serve as the primary benchmark, measuring the percentage of games won out of the total games played. This metric quantifies the overall success of each AI strategy.  \n",
    "Win Rate = Number of Games Won / Total Games Played x 100%\n",
    "\n",
    "* To assess efficiency, we will use the average game completion time, which calculates the mean duration (in seconds) taken to finish a game, whether won or lost. \n",
    "\n",
    "* The exploration rate will evaluate how thoroughly the AI reveals non-mine tiles, serving as an indicator of its exploration strategy.  \n",
    "Exploration Rate = Non-Mine Tiles Revealed / Total Non-Mine Tiles x 100%\n",
    "\n",
    "Together, these quantifiable metrics will provide a comparison of the performance between the benchmark and proposed AI models, and the best performing model will be our solution model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our project does not involve sensitive user data, there are several ethical considerations to take into account:\n",
    "\n",
    "#### **1. Algorithmic Transparency and Explainability**\n",
    "AI-driven Minesweeper solvers can act as a testbed for more complex decision-making systems. Ensuring that the decision-making process remains interpretable and explainable is crucial, particularly when applying similar methodologies in real-world scenarios such as medical diagnosis or risk assessment. Black-box AI models can lead to unpredictable or untrustworthy behavior in safety-critical applications (Lipton, 2018)[5].",
    "\n",
    "#### **2. Unintended Bias in AI Decision-Making**\n",
    "Although Minesweeper is a well-defined game with no external biases, the algorithms developed for solving it may have inherent biases due to their reliance on heuristic approximations. For instance, certain algorithms may favor safer strategies that prioritize known information over exploration, potentially leading to suboptimal long-term performance. This consideration is particularly relevant when extending these AI techniques to real-world applications such as autonomous systems or financial modeling.\n",
    "\n",
    "#### **3. Computational Resource Consumption**\n",
    "AI models, particularly those using Monte Carlo simulations or Reinforcement Learning, require significant computational resources for training and execution. Excessive resource consumption has environmental and ethical implications, especially given concerns over the carbon footprint of large-scale machine learning models (Strubell et al., 2019)[6]. To address this, our implementation will focus on optimizing computational efficiency by using targeted simulations and avoiding unnecessary computations.\n",
    "\n",
    "#### **4. Broader Ethical Implications of AI in Decision-Making**\n",
    "The AI methodologies applied in this project—such as CSP, Monte Carlo simulations, and RL—are widely used in fields such as finance, healthcare, and security. While our project is focused on Minesweeper, similar AI decision-making techniques could be applied in high-stakes scenarios where incorrect or biased decisions could have real-world consequences. Ensuring the ethical development of AI systems and recognizing their broader impact is a fundamental responsibility of AI researchers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reply to team messages within 24 hours to ensure effective communication.\n",
    "* Attend weekly team meetings and actively participate in discussions.\n",
    "* Be respectful to every teammate—value each other’s opinions, backgrounds, and contributions.\n",
    "* Communicate early and openly about any conflicts or difficulties; work together to find constructive solutions.\n",
    "* Make decisions collaboratively, ensuring every voice is heard. When consensus cannot be reached, vote or defer to the project lead’s decision.\n",
    "* Divide tasks fairly and take responsibility for individual contributions.\n",
    "* Offer and accept constructive feedback with an open mind.\n",
    "* Notify the team in advance if you cannot meet a deadline or attend a meeting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/17  |  7 PM |  Complete project proposal  | Discuss potential AI solutions to our problem | \n",
    "| 2/24  |  7 PM |  Refine project proposal | Discuss potential implementation | \n",
    "| 3/3  | 7 PM  | Develop individial solutions  | Discuss individial solutions and evaluation metrics  |\n",
    "| 3/10  | 7 PM  | Refine solutions | Review potential errors and work on report   |\n",
    "| 3/17  | 7 PM  | Finalize report | Review/edit project report |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
